{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "# import pathlib\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import language_tool_python\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileName=input()\n",
    "\n",
    "# with pymupdf.open(fileName) as doc:\n",
    "#     text=chr(12).join([page.get_text() for page in doc])\n",
    "\n",
    "# pathlib.Path(fileName+\".txt\").write_bytes(text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pymupdf.open(pdf_path) as pdf:\n",
    "        text=\"\"\n",
    "        for page_num in range(len(pdf)):\n",
    "            page=pdf[page_num]\n",
    "            text+=page.get_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to extract PDFs from zipfile, convert them to text, and save as .txt files\n",
    "# def extract_and_save_text_from_zip(zip_file_path, extract_to, output_zip_path):\n",
    "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(extract_to)\n",
    "\n",
    "#     # Create a new zip file to store the .txt files\n",
    "#     with zipfile.ZipFile(output_zip_path, 'w') as output_zip:\n",
    "#         for root, dirs, files in os.walk(extract_to):\n",
    "#             for file in files:\n",
    "#                 if file.endswith(\".pdf\"):\n",
    "#                     pdf_path = os.path.join(root, file)\n",
    "#                     text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "#                     # Save the extracted text as a .txt file\n",
    "#                     text_filename = os.path.splitext(file)[0] + \".txt\"\n",
    "#                     text_filepath = os.path.join(extract_to, text_filename)\n",
    "#                     with open(text_filepath, 'w', encoding='utf-8') as text_file:\n",
    "#                         text_file.write(text)\n",
    "\n",
    "#                     # Add the text file to the output zip\n",
    "#                     output_zip.write(text_filepath, arcname=text_filename)\n",
    "\n",
    "# # Example usage\n",
    "# zip_file_path = \"Final_Resumes.zip\"  # Path to your input zipfile containing PDFs\n",
    "# extract_to = \"Final_Resumes_Text\"  # Temporary directory to extract PDFs\n",
    "# output_zip_path = \"extracted_text_files.zip\"  # Path to the output zipfile with text files\n",
    "\n",
    "# # Extract text and create a zip of .txt files\n",
    "# extract_and_save_text_from_zip(zip_file_path, extract_to, output_zip_path)\n",
    "\n",
    "# print(f\"Text files saved and zipped as {output_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(directory):\n",
    "    \"\"\"Load data from text files in the specified directory and perform initial cleaning.\"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"Resume_of_ID_\") and filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            id_number = int(filename.split('_')[3].split('.')[0])\n",
    "            # id_number = int(filename.split('_')[1].split('.')[0])\n",
    "            clean_text = ' '.join(text.split())  # Remove extra whitespace\n",
    "            data.append({'ID': id_number, 'Text': clean_text})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text using spaCy for tokenization and lemmatization.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years_of_experience(text):\n",
    "    \"\"\"Extract years of experience from the resume text.\"\"\"\n",
    "    years = re.findall(r'\\b(19[7-9]\\d|20[0-2]\\d)\\b', text)\n",
    "    if len(years) >= 2:\n",
    "        earliest_year = min(int(year) for year in years)\n",
    "        latest_year = max(int(year) for year in years)\n",
    "        current_year = datetime.datetime.now().year\n",
    "        if latest_year > current_year:\n",
    "            latest_year = current_year\n",
    "        return latest_year - earliest_year\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_education_level(text):\n",
    "    \"\"\"Detect the highest education level mentioned in the resume.\"\"\"\n",
    "    education_patterns = {\n",
    "        'PhD': r'\\bPh\\.?D\\.?\\b|\\bDoctor(ate)?\\b',\n",
    "        'Master': r'\\bM\\.?S\\.?\\b|\\bM\\.?A\\.?\\b|\\bMaster(s)?\\b',\n",
    "        'Bachelor': r'\\bB\\.?S\\.?\\b|\\bB\\.?A\\.?\\b|\\bBachelor(s)?\\b',\n",
    "        'Associate': r'\\bA\\.?S\\.?\\b|\\bA\\.?A\\.?\\b|\\bAssociate\\b'\n",
    "    }\n",
    "\n",
    "    for level, pattern in education_patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return level\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spell_check_ratio(text):\n",
    "    \"\"\"Calculate the ratio of potential spelling errors to total words.\"\"\"\n",
    "    matches = language_tool_python.LanguageTool('en-US').check(text)\n",
    "    total_words = len(text.split())\n",
    "    return 1 - (len(matches) / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_resume_sections(text):\n",
    "    \"\"\"Identify and score the presence of important resume sections.\"\"\"\n",
    "    important_sections = ['education', 'experience', 'skills', 'projects', 'achievements']\n",
    "    optional_sections = ['summary', 'objective', 'interests', 'activities']\n",
    "    unnecessary_sections = ['references']\n",
    "\n",
    "    section_score = 0\n",
    "    for section in important_sections:\n",
    "        if re.search(r'\\b' + section + r'\\b', text, re.IGNORECASE):\n",
    "            section_score += 1\n",
    "\n",
    "    for section in optional_sections:\n",
    "        if re.search(r'\\b' + section + r'\\b', text, re.IGNORECASE):\n",
    "            section_score += 0.5\n",
    "\n",
    "    for section in unnecessary_sections:\n",
    "        if re.search(r'\\b' + section + r'\\b', text, re.IGNORECASE):\n",
    "            section_score -= 0.5\n",
    "\n",
    "    return min(section_score / len(important_sections), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_brevity(text):\n",
    "    \"\"\"Quantify the brevity of the resume.\"\"\"\n",
    "    word_count = len(text.split())\n",
    "    if word_count < 200:\n",
    "        return 0.5  # Too short\n",
    "    elif word_count > 1000:\n",
    "        return 0.5  # Too long\n",
    "    else:\n",
    "        return 1 - (abs(600 - word_count) / 400)  # Optimal around 600 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(row):\n",
    "    \"\"\"Process a single resume and return a dictionary of features.\"\"\"\n",
    "    text = row['Text']\n",
    "    tokens = preprocess_text(text)\n",
    "\n",
    "    years_of_experience = extract_years_of_experience(text)\n",
    "    education_level = detect_education_level(text)\n",
    "    spell_check_ratio = calculate_spell_check_ratio(text)\n",
    "    section_score = identify_resume_sections(text)\n",
    "    brevity_score = quantify_brevity(text)\n",
    "\n",
    "    # Generate job description based on the resume\n",
    "    job_description = generate_job_description(text)\n",
    "\n",
    "    return {\n",
    "        'ID': row['ID'],\n",
    "        'Preprocessed_Tokens': tokens,\n",
    "        'Years_of_Experience': years_of_experience,\n",
    "        'Education_Level': education_level,\n",
    "        'Spell_Check_Ratio': spell_check_ratio,\n",
    "        'Section_Score': section_score,\n",
    "        'Brevity_Score': brevity_score,\n",
    "        'Generated_Job_Description': job_description\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_sentence_counts(text):\n",
    "    \"\"\"Calculate word count and sentence count.\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = len([s for s in sentences if s.strip()])\n",
    "    return word_count, sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(text, skill_list):\n",
    "    \"\"\"Extract skills from text based on a predefined skill list.\"\"\"\n",
    "    found_skills = [skill for skill in skill_list if re.search(r'\\b' + re.escape(skill) + r'\\b', text, re.IGNORECASE)]\n",
    "    return found_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skill_match_score(resume_skills, job_skills):\n",
    "    \"\"\"Calculate the skill match score.\"\"\"\n",
    "    if not job_skills:\n",
    "        return 0\n",
    "    matched_skills = set(resume_skills) & set(job_skills)\n",
    "    return len(matched_skills) / len(job_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze the sentiment of achievement statements in the resume.\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_achievement_impact(text):\n",
    "    \"\"\"Quantify the impact of achievements.\"\"\"\n",
    "    impact_score = 0\n",
    "    achievements = re.findall(r'\\b(increased|decreased|improved|reduced|saved|generated).*?(\\d+(?:\\.\\d+)?%?)', text, re.IGNORECASE)\n",
    "    for _, value in achievements:\n",
    "        if '%' in value:\n",
    "            impact_score += float(value.strip('%')) / 100\n",
    "        else:\n",
    "            impact_score += float(value) / 1000  # Assume larger numbers for non-percentage values\n",
    "    return min(impact_score, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_technical_score(row):\n",
    "    \"\"\"Calculate the technical CV score.\"\"\"\n",
    "    skill_count = len(row['Extracted_Skills'])\n",
    "    experience_score = min(row['Years_of_Experience'] / 10, 1)  # Cap at 10 years\n",
    "    education_score = {'PhD': 1, 'Master': 0.8, 'Bachelor': 0.6, 'Associate': 0.4, 'Other': 0.2}.get(row['Education_Level'], 0.2)\n",
    "\n",
    "    return (skill_count / 10 * 0.4 + experience_score * 0.3 + education_score * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_managerial_score(row):\n",
    "    \"\"\"Calculate the managerial CV score.\"\"\"\n",
    "    soft_skills_score = analyze_sentiment(row['Text'])\n",
    "    achievement_impact = quantify_achievement_impact(row['Text'])\n",
    "    leadership_score = min(row['Years_of_Experience'] / 15, 1)  # Assume leadership potential increases with experience\n",
    "\n",
    "    return (soft_skills_score * 0.3 + achievement_impact * 0.4 + leadership_score * 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_score(row):\n",
    "    \"\"\"Calculate the overall CV score.\"\"\"\n",
    "    technical_score = row['Technical_Score']\n",
    "    managerial_score = row['Managerial_Score']\n",
    "    resume_quality_score = (row['Spell_Check_Ratio'] + row['Section_Score'] + row['Brevity_Score']) / 3\n",
    "    return (technical_score * 0.4 + managerial_score * 0.3 + resume_quality_score * 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume_section2(row, job_skills):\n",
    "    \"\"\"Process a single resume for Section 2 analysis.\"\"\"\n",
    "    word_count, sentence_count = calculate_word_sentence_counts(row['Text'])\n",
    "\n",
    "    # Combine general and technical skills\n",
    "    all_skills = job_skills['general_skills'] + job_skills['technical_skills']\n",
    "\n",
    "    # Extract skills based on the generated job description\n",
    "    job_specific_skills = extract_skills(row['Generated_Job_Description'], all_skills)\n",
    "    resume_skills = extract_skills(row['Text'], all_skills)\n",
    "\n",
    "    technical_score = calculate_technical_score(row, job_specific_skills)\n",
    "    managerial_score = calculate_managerial_score(row)\n",
    "    resume_quality_score = (row['Spell_Check_Ratio'] + row['Section_Score'] + row['Brevity_Score']) / 3\n",
    "    overall_score = calculate_overall_score(technical_score, managerial_score, resume_quality_score)\n",
    "\n",
    "    return {\n",
    "        'ID': row['ID'],\n",
    "        'Word_Count': word_count,\n",
    "        'Sentence_Count': sentence_count,\n",
    "        'Resume_Skills': resume_skills,\n",
    "        'Job_Specific_Skills': job_specific_skills,\n",
    "        'Technical_Score': technical_score,\n",
    "        'Managerial_Score': managerial_score,\n",
    "        'Resume_Quality_Score': resume_quality_score,\n",
    "        'Overall_Score': overall_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(df):\n",
    "    \"\"\"Normalize scores to ensure fair comparison across all resumes.\"\"\"\n",
    "    score_columns = ['Technical_Score', 'Managerial_Score', 'Resume_Quality_Score', 'Overall_Score']\n",
    "    for column in score_columns:\n",
    "        df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resumes_section2(df, job_skills):\n",
    "    \"\"\"Process all resumes for Section 2 analysis.\"\"\"\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        results.append(process_resume_section2(row, job_skills))\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    normalized_df = normalize_scores(results_df)\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be called from the main function in Section 3\n",
    "def run_section2(input_file, job_skills):\n",
    "    \"\"\"Run Section 2 processing on the input file.\"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    processed_df = process_resumes_section2(df, job_skills)\n",
    "    processed_df.to_csv('processed_resumes_section2.csv', index=False)\n",
    "    print(\"Section 2 processing completed. Results saved to 'processed_resumes_section2.csv'\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_job_skills(file_path: str) -> List[str]:\n",
    "    \"\"\"Load job skills from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_description_matching(resume_text: str, job_description: str) -> float:\n",
    "    \"\"\"Calculate similarity between resume and job description.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([resume_text, job_description])\n",
    "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_scores_with_job_match(df: pd.DataFrame, job_description: str) -> pd.DataFrame:\n",
    "    \"\"\"Adjust scores based on job description matching.\"\"\"\n",
    "    df['Job_Match_Score'] = df['Text'].apply(lambda x: job_description_matching(x, job_description))\n",
    "    df['Adjusted_Overall_Score'] = (df['Overall_Score'] * 0.7 + df['Job_Match_Score'] * 0.3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_resumes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rank resumes based on adjusted overall score.\"\"\"\n",
    "    return df.sort_values('Adjusted_Overall_Score', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(df: pd.DataFrame, top_n: int = 10) -> Dict:\n",
    "    \"\"\"Generate a report with top candidates and summary statistics.\"\"\"\n",
    "    top_candidates = df.head(top_n)[['ID', 'Adjusted_Overall_Score', 'Technical_Score', 'Managerial_Score', 'Job_Match_Score']]\n",
    "    summary_stats = df[['Adjusted_Overall_Score', 'Technical_Score', 'Managerial_Score', 'Job_Match_Score']].describe()\n",
    "\n",
    "    return {\n",
    "        'top_candidates': top_candidates.to_dict(orient='records'),\n",
    "        'summary_stats': summary_stats.to_dict()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_resume_to_job_description(resume_text, job_description):\n",
    "    \"\"\"Match a resume to a specific job description and return adjusted scores.\"\"\"\n",
    "    # Reuse the existing job_description_matching function\n",
    "    match_score = job_description_matching(resume_text, job_description)\n",
    "\n",
    "    # Recalculate scores based on the new job description\n",
    "    job_specific_skills = extract_skills(job_description, general_job_skills)\n",
    "    resume_skills = extract_skills(resume_text, job_specific_skills)\n",
    "\n",
    "    technical_score = calculate_technical_score({'Text': resume_text, 'Years_of_Experience': extract_years_of_experience(resume_text), 'Education_Level': detect_education_level(resume_text)}, job_specific_skills)\n",
    "    managerial_score = calculate_managerial_score({'Text': resume_text})\n",
    "    resume_quality_score = (calculate_spell_check_ratio(resume_text) + identify_resume_sections(resume_text) + quantify_brevity(resume_text)) / 3\n",
    "    overall_score = calculate_overall_score(technical_score, managerial_score, resume_quality_score)\n",
    "\n",
    "    adjusted_overall_score = overall_score * 0.7 + match_score * 0.3\n",
    "\n",
    "    return {\n",
    "        'Technical_Score': technical_score,\n",
    "        'Managerial_Score': managerial_score,\n",
    "        'Resume_Quality_Score': resume_quality_score,\n",
    "        'Overall_Score': overall_score,\n",
    "        'Job_Match_Score': match_score,\n",
    "        'Adjusted_Overall_Score': adjusted_overall_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_job_skills_file(file_path):\n",
    "    \"\"\"Create a default job_skills.json file if it doesn't exist.\"\"\"\n",
    "    default_skills = {\n",
    "        \"general_skills\": [\n",
    "            \"communication\", \"teamwork\", \"leadership\", \"problem-solving\",\n",
    "            \"time management\", \"analytical skills\", \"creativity\", \"adaptability\"\n",
    "        ],\n",
    "        \"technical_skills\": [\n",
    "            \"programming\", \"data analysis\", \"project management\",\n",
    "            \"software development\", \"database management\", \"web development\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(default_skills, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_job_skills(file_path: str) -> List[str]:\n",
    "    \"\"\"Load general job skills from a JSON file or use a default list.\"\"\"\n",
    "    default_skills = [\n",
    "        \"communication\", \"teamwork\", \"leadership\", \"problem-solving\",\n",
    "        \"time management\", \"analytical skills\", \"creativity\", \"adaptability\",\n",
    "        \"organization\", \"attention to detail\", \"customer service\",\n",
    "        \"critical thinking\", \"decision making\", \"interpersonal skills\",\n",
    "        \"multitasking\", \"flexibility\", \"initiative\", \"reliability\",\n",
    "        \"professionalism\", \"continuous learning\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                skills_data = json.load(file)\n",
    "            if isinstance(skills_data, list):\n",
    "                return skills_data\n",
    "            elif isinstance(skills_data, dict) and 'skills' in skills_data:\n",
    "                return skills_data['skills']\n",
    "            else:\n",
    "                print(f\"Unexpected format in '{file_path}'. Using default skills list.\")\n",
    "                return default_skills\n",
    "        else:\n",
    "            print(f\"'{file_path}' not found. Using default skills list.\")\n",
    "            return default_skills\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading '{file_path}'. Using default skills list.\")\n",
    "        return default_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(text: str) -> List[str]:\n",
    "    \"\"\"Extract skills from text using NLP techniques.\"\"\"\n",
    "    # This is a placeholder function. In a real implementation, you would use\n",
    "    # more sophisticated NLP techniques to extract skills from the text.\n",
    "    # For now, we'll use a simple keyword matching approach.\n",
    "    extracted_skills = []\n",
    "    for skill in general_skills:\n",
    "        if skill.lower() in text.lower():\n",
    "            extracted_skills.append(skill)\n",
    "    return extracted_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(row):\n",
    "    \"\"\"Process a single resume and return a dictionary of features.\"\"\"\n",
    "    text = row['Text']\n",
    "\n",
    "    years_of_experience = extract_years_of_experience(text)\n",
    "    education_level = detect_education_level(text)\n",
    "    spell_check_ratio = calculate_spell_check_ratio(text)\n",
    "    section_score = identify_resume_sections(text)\n",
    "    brevity_score = quantify_brevity(text)\n",
    "    extracted_skills = extract_skills(text)\n",
    "\n",
    "    return {\n",
    "        'ID': row['ID'],\n",
    "        'Years_of_Experience': years_of_experience,\n",
    "        'Education_Level': education_level,\n",
    "        'Spell_Check_Ratio': spell_check_ratio,\n",
    "        'Section_Score': section_score,\n",
    "        'Brevity_Score': brevity_score,\n",
    "        'Extracted_Skills': extracted_skills\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(resume_directory: str):\n",
    "    print(\"Starting resume analysis process...\")\n",
    "\n",
    "    # Load general skills\n",
    "    global general_skills\n",
    "    general_skills = load_job_skills('job_skills.json')\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing resumes...\")\n",
    "    df = load_and_clean_data(resume_directory)\n",
    "    df['processed'] = df.apply(process_resume, axis=1)\n",
    "    df = pd.concat([df, pd.DataFrame(df['processed'].tolist())], axis=1)\n",
    "    df.drop('processed', axis=1, inplace=True)\n",
    "\n",
    "    # Calculate scores\n",
    "    df['Skill_Count'] = df['Extracted_Skills'].apply(len)\n",
    "    df['Technical_Score'] = df.apply(calculate_technical_score, axis=1)\n",
    "    df['Managerial_Score'] = df.apply(calculate_managerial_score, axis=1)\n",
    "    df['Overall_Score'] = df.apply(calculate_overall_score, axis=1)\n",
    "\n",
    "    # Rank resumes\n",
    "    print(\"Ranking resumes...\")\n",
    "    ranked_df = df.sort_values('Overall_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Save final results\n",
    "    final_columns = [\n",
    "        'ID', 'Years_of_Experience', 'Education_Level', 'Spell_Check_Ratio',\n",
    "        'Section_Score', 'Brevity_Score', 'Skill_Count',\n",
    "        'Technical_Score', 'Managerial_Score', 'Overall_Score', 'Extracted_Skills'\n",
    "    ]\n",
    "    ranked_df[final_columns].to_csv('final_ranked_resumes.csv', index=False)\n",
    "\n",
    "    print(\"Resume analysis complete. Results saved to 'final_ranked_resumes.csv'\")\n",
    "\n",
    "    return ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting resume analysis process...\n",
      "'job_skills.json' not found. Using default skills list.\n",
      "Loading and preprocessing resumes...\n",
      "Ranking resumes...\n",
      "Resume analysis complete. Results saved to 'final_ranked_resumes.csv'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    resume_directory = '/workspaces/ResumeAnalyser/extracted_text_files'\n",
    "    main(resume_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
